{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Wine Quality.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _\"Quality ratings of Portuguese white wines\" (Classification task)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "## Part 0: Introduction\n",
    "\n",
    "### Overview\n",
    "The dataset that's we see here contains 12 columns and 4898 entries of data about Portuguese white wines.\n",
    "    \n",
    "**Метаданные:**\n",
    "    \n",
    "* **fixed acidity** \n",
    "\n",
    "* **volatile acidity**\n",
    "\n",
    "* **citric acid** \n",
    "\n",
    "* **residual sugar** \n",
    "\n",
    "* **chlorides** \n",
    "\n",
    "* **free sulfur dioxide** \n",
    "\n",
    "* **total sulfur dioxide**\n",
    "\n",
    "* **density** \n",
    "\n",
    "* **pH** \n",
    "\n",
    "* **sulphates** \n",
    "\n",
    "* **alcohol** \n",
    "\n",
    "* **quality** - score between 3 and 9\n",
    "\n",
    "\n",
    "### Questions:\n",
    "    \n",
    "Predict which wines are 'Good/1' and 'Not Good/0' (use binary classification; check balance of classes; calculate perdictions; choose the best model)\n",
    "\n",
    "\n",
    "## [Part 1: Import, Load Data](#Part-1:-Import,-Load-Data.)\n",
    "* ### Import libraries, Read data from ‘.csv’ file\n",
    "\n",
    "## [Part 2: Exploratory Data Analysis](#Part-2:-Exploratory-Data-Analysis.)\n",
    "* ### Info, Head, Describe\n",
    "* ### Encoding 'quality' attribute\n",
    "* ### 'quality' attribute value counts and visualisation\n",
    "* ### Resampling of an imbalanced dataset\n",
    "* ### Random under-sampling of an imbalanced dataset\n",
    "* ### Random over-sampling of an imbalanced dataset\n",
    "* ### Initialisation of target\n",
    "* ### Drop column 'quality'\n",
    "\n",
    "## [Part 3: Data Wrangling and Transformation](#Part-3:-Data-Wrangling-and-Transformation.)\n",
    "* ### StandardScaler\n",
    "* ### Creating datasets for ML part\n",
    "* ### 'Train\\Test' splitting method\n",
    "\n",
    "## [Part 4: Machine Learning](#Part-4:-Machine-Learning.)\n",
    "* ### Build, train and evaluate models without hyperparameters\n",
    "    * #### Logistic Regression, K-Nearest Neighbors, Decision Trees \n",
    "    * #### Classification report\n",
    "    * #### Confusion Matrix\n",
    "    * #### ROC-AUC score\n",
    "* ### Build, train and evaluate models with hyperparameters\n",
    "    * #### Logistic Regression, K-Nearest Neighbors, Decision Trees \n",
    "    * #### Classification report\n",
    "    * #### Confusion Matrix\n",
    "    * #### ROC-AUC score\n",
    "\n",
    "## [Conclusion](#Conclusion.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Import, Load Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Read data from ‘.csv’ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from '.csv' file\n",
    "dataset = pd.read_csv('winequality.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the full summary of the dataset  \n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview of the first 5 lines of the loaded data \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, вам дали такой датасет и поставили конктетный вопрос: классифицируйте какие вина хорошие, а какие нет? У вас нет атрибута \"Y\" и ответа. Но есть хороший вспомогательный атрибут \"quality\" из которого мы сможем создать наш атрибут \"Y\" с ответом для обучения модели. Атрибут \"quality\" имеет значения от 3 до 9, где 3 это \"Not Good\", а 9 это \"Good\" качество вина. Чем выше число, тем лучше вино."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Encoding 'quality' attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda function; wine quality from 3-6 == 0, from 7-9 == 1.\n",
    "dataset['quality'] = dataset.quality.apply(lambda q: 0 if q <= 6 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview of the first 5 lines of the loaded data \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### 'quality' attribute value counts and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Not good wine', round(dataset['quality'].value_counts()[0]/len(dataset) * 100,2), '% of the dataset')\n",
    "print('Good wine', round(dataset['quality'].value_counts()[1]/len(dataset) * 100,2), '% of the dataset')\n",
    "\n",
    "dataset['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualisation plot\n",
    "dataset['quality'].value_counts().plot(x = dataset['quality'], kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Resampling of an imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class count\n",
    "#count_class_0, count_class_1 = dataset.quality.value_counts()\n",
    "\n",
    "# divide by class\n",
    "#class_0 = dataset[dataset['quality'] == 0]\n",
    "#class_1 = dataset[dataset['quality'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Random under-sampling of an imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_0_under = class_0.sample(count_class_1)\n",
    "#dataset_under = pd.concat([class_0_under, class_1], axis=0)\n",
    "\n",
    "#print('Random under-sampling:')\n",
    "#print(dataset_under.quality.value_counts())\n",
    "\n",
    "#dataset_under.quality.value_counts().plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Random over-sampling of an imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_1_over = class_1.sample(count_class_0, replace=True)\n",
    "#dataset_over = pd.concat([class_0, class_1_over], axis=0)\n",
    "\n",
    "#print('Random over-sampling:')\n",
    "#print(dataset_over.quality.value_counts())\n",
    "\n",
    "#dataset_over.quality.value_counts().plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Initialisation of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation of target\n",
    "target = dataset['quality']\n",
    "\n",
    "# for under-sampling dataset\n",
    "#target_under = dataset_under['quality']\n",
    "\n",
    "# for over-sampling dataset\n",
    "#target_over = dataset_over['quality'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Drop column 'quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=['quality'])\n",
    "\n",
    "# for under-sampling dataset\n",
    "#dataset_under = dataset_under.drop(columns=['quality'])\n",
    "\n",
    "# for over-sampling dataset\n",
    "#dataset_over = dataset_over.drop(columns=['quality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Wrangling and Transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler \n",
    "sc = StandardScaler()\n",
    "\n",
    "dataset_sc = sc.fit_transform(dataset)\n",
    "\n",
    "# for under-sampling dataset\n",
    "#dataset_sc = sc.fit_transform(dataset_under)\n",
    "\n",
    "# for over-sampling dataset\n",
    "#dataset_sc = sc.fit_transform(dataset_over)\n",
    "\n",
    "dataset_sc = pd.DataFrame(dataset_sc)\n",
    "dataset_sc.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Creating datasets for ML part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 'X' for features' and y' for the target ('quality').\n",
    "y = target\n",
    "X = dataset_sc.copy()\n",
    "\n",
    "# for under-sampling dataset \n",
    "#y = target_under\n",
    "#X = dataset_sc.copy()\n",
    "\n",
    "# for over-sampling dataset \n",
    "#y = target_over\n",
    "#X = dataset_sc.copy() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview of the first 5 lines of the loaded data \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### 'Train\\Test' split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply 'Train\\Test' splitting method\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shape of X_train and y_train\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print shape of X_test and y_test\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Build, train and evaluate models without hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression\n",
    "* K-Nearest Neighbors\n",
    "* Decision Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "LR_pred = LR.predict(X_test)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(X_train, y_train)\n",
    "KNN_pred = KNN.predict(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "DT = DecisionTreeClassifier(random_state = 0)\n",
    "DT.fit(X_train, y_train)\n",
    "DT_pred = DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"LR Classification Report: \\n\", classification_report(y_test, LR_pred, digits = 6))\n",
    "print(\"KNN Classification Report: \\n\", classification_report(y_test, KNN_pred, digits = 6))\n",
    "print(\"DT Classification Report: \\n\", classification_report(y_test, DT_pred, digits = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_confusion_mx = confusion_matrix(y_test, LR_pred)\n",
    "print(\"LR Confusion Matrix: \\n\", LR_confusion_mx)\n",
    "print()\n",
    "KNN_confusion_mx = confusion_matrix(y_test, KNN_pred)\n",
    "print(\"KNN Confusion Matrix: \\n\", KNN_confusion_mx)\n",
    "print()\n",
    "DT_confusion_mx = confusion_matrix(y_test, DT_pred)\n",
    "print(\"DT Confusion Matrix: \\n\", DT_confusion_mx)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### ROC-AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roc_auc_score(DT_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Build, train and evaluate models with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "LR = LogisticRegression()\n",
    "LR_params = {'C':[1,2,3,4,5,6,7,8,9,10], 'penalty':['l1', 'l2', 'elasticnet', 'none'], 'solver':['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'], 'random_state':[0]}\n",
    "LR1 = GridSearchCV(LR, param_grid = LR_params)\n",
    "LR1.fit(X_train, y_train)\n",
    "LR1_pred = LR1.predict(X_test)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN_params = {'n_neighbors':[5,7,9,11]}\n",
    "KNN1 = GridSearchCV(KNN, param_grid = KNN_params)             \n",
    "KNN1.fit(X_train, y_train)\n",
    "KNN1_pred = KNN1.predict(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "DT = DecisionTreeClassifier()\n",
    "DT_params = {'max_depth':[2,10,15,20], 'criterion':['gini', 'entropy'], 'random_state':[0]}\n",
    "DT1 = GridSearchCV(DT, param_grid = DT_params)\n",
    "DT1.fit(X_train, y_train)\n",
    "DT1_pred = DT1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best hyper parameters set\n",
    "print(\"Logistic Regression Best Hyper Parameters:   \", LR1.best_params_)\n",
    "print(\"K-Nearest Neighbour Best Hyper Parameters:   \", KNN1.best_params_)\n",
    "print(\"Decision Tree Best Hyper Parameters:         \", DT1.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LR Classification Report: \\n\", classification_report(y_test, LR1_pred, digits = 6))\n",
    "print(\"KNN Classification Report: \\n\", classification_report(y_test, KNN1_pred, digits = 6))\n",
    "print(\"DT Classification Report: \\n\", classification_report(y_test, DT1_pred, digits = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix of DT model\n",
    "DT_confusion_mx = confusion_matrix(y_test, DT1_pred)\n",
    "print('DT Confusion Matrix')\n",
    "\n",
    "# visualisation\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(DT_confusion_mx, annot = True, fmt = 'd', cmap = 'Blues', ax = ax, linewidths = 0.5, annot_kws = {'size': 15})\n",
    "ax.set_ylabel('FP       True label        TP')\n",
    "ax.set_xlabel('FN       Predicted label        TN')\n",
    "ax.xaxis.set_ticklabels(['1', '0'], fontsize = 10)\n",
    "ax.yaxis.set_ticklabels(['1', '0'], fontsize = 10)\n",
    "plt.show()\n",
    "print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### ROC-AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(DT1_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission of .csv file with predictions\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = X_test.index\n",
    "sub['quality'] = DT1_pred\n",
    "sub.to_csv('WinePredictionsTest.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Predict which wines are 'Good/1' and 'Not Good/0' (use binary classification; check balance of classes; calculate perdictions; choose the best model).\n",
    "\n",
    "Answers:\n",
    "\n",
    "   1. Binary classification was applied.\n",
    "\n",
    "   2. Classes were highly imbalanced with 78.36 % of '0' class and only 21.64 % of '1' class in our dataset.\n",
    "\n",
    "   3. Three options were applied in order to calculate the best predictions:\n",
    "      -  Calculate predictions with imbalanced dataset\n",
    "      -  Calculate predictions with random under-sampling technique of an imbalanced dataset\n",
    "      -  Calculate predictions with random over-sampling technique of an imbalanced dataset\n",
    "\n",
    "   4. Three ML models were used: Logistic Regression, KNN, Decision Tree (without and with hyper parameters).\n",
    "\n",
    "   5. The best result was choosen:\n",
    "       - Random over-sampling dataset with 3838 enteties in class '0' and 3838 enteties in class '1', 7676 enteties in total.\n",
    "       - Train/Test split: test_size=0.2, random_state=0\n",
    "       - Decision Tree model without hyper parameters tuning, with an accuracy score equal ... and ROC-AUC score equal ... .\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
